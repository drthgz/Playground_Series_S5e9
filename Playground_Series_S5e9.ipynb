{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91720,"databundleVersionId":13345277,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":10.17418,"end_time":"2025-09-08T04:52:09.151814","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-08T04:51:58.977634","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/syedfarazhussaini/playground-series-s5e9?scriptVersionId=260775068\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/syedfarazhussaini/playground-series-s5e9?scriptVersionId=260768283\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.354273,"end_time":"2025-09-08T04:52:08.526426","exception":false,"start_time":"2025-09-08T04:52:06.172153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step-by-Step Guide: Predicting the Beats-per-Minute of Songs\nWelcome to the Kaggle Playground Series S5E9! This notebook will guide you through the process of building a model to predict the beats-per-minute (BPM) of songs.\n\n## Steps\n1. **Understand the Problem**  \n   - Review the competition goal and data format.\n2. **Import Libraries & Load Data**  \n   - Import necessary Python libraries.\n   - Load the training and test datasets.\n3. **Explore the Data (EDA)**  \n   - Inspect the data structure, check for missing values, and visualize distributions.\n4. **Preprocess the Data**  \n   - Handle missing values, encode categorical variables, and scale features if needed.\n5. **Build a Baseline Model**  \n   - Train a simple regression model (e.g., Linear Regression, Random Forest, or XGBoost).\n6. **Evaluate the Model**  \n   - Use cross-validation or a validation split to assess performance.\n7. **Feature Engineering & Model Improvement**  \n   - Try new features, different models, or hyperparameter tuning to improve results.\n8. **Make Predictions on Test Data**  \n   - Generate predictions for the test set.\n9. **Prepare Submission**  \n   - Format predictions according to `sample_submission.csv` and save for submission.\n10. **Submit to Kaggle**  \n   - Upload your submission and review your score.\n\nLet's get started!","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Libraries & Load Data\nLet's import the necessary libraries and load the training, test, and sample submission datasets.","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\n\n# Load datasets\nfilePath = \"/kaggle/input/playground-series-s5e9\"\ntrain = pd.read_csv(f'{filePath}/train.csv')\ntest = pd.read_csv(f'{filePath}/test.csv')\nsample_submission = pd.read_csv(f'{filePath}/sample_submission.csv')\n\n# Show the shape of the datasets\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('Sample submission shape:', sample_submission.shape)\n\n# Display the first few rows of the training data\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Explore the Data (EDA)\nLet's start exploring the data. We'll begin by checking the structure, types, and summary statistics of the training set.","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Data Structure and Types\nLet's check the columns, data types, and missing values in the training data.","metadata":{}},{"cell_type":"code","source":"# Show columns and data types\nprint(train.dtypes)\n\n# Check for missing values\nprint('\\nMissing values per column:')\nprint(train.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2. Summary Statistics\nNow, let's look at summary statistics for the numeric features in the training data.","metadata":{}},{"cell_type":"code","source":"# Summary statistics for numeric columns\ntrain.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3. Target Variable Distribution\nLet's visualize the distribution of the target variable (BPM) to understand its range and shape.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot the distribution of the target variable (assuming 'BeatsPerMinute' is the target column)\nplt.figure(figsize=(8, 4))\nsns.histplot(train['BeatsPerMinute'], kde=True, bins=30)\nplt.title('Distribution of BPM (Target Variable)')\nplt.xlabel('BPM')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4. Feature Distributions and Relationships\nLet's visualize the distributions of some features and their relationships with the target variable (BPM).","metadata":{}},{"cell_type":"code","source":"# Plot distributions for a few numeric features and their relationship with BPM\nnumeric_features = train.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_features = [f for f in numeric_features if f != 'BeatsPerMinute']  # Exclude target\nsample_features = numeric_features[:3]  # Plot first 3 features as example\n\nfig, axes = plt.subplots(len(sample_features), 2, figsize=(12, 4 * len(sample_features)))\nfor i, feature in enumerate(sample_features):\n    # Distribution\n    sns.histplot(train[feature], kde=True, ax=axes[i, 0])\n    axes[i, 0].set_title(f'Distribution of {feature}')\n    # Relationship with BeatsPerMinute\n    sns.scatterplot(x=train[feature], y=train['BeatsPerMinute'], ax=axes[i, 1], alpha=0.3)\n    axes[i, 1].set_title(f'{feature} vs BeatsPerMinute')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Preprocess the Data\nLet's prepare the data for modeling. We'll start by handling missing values, then encode categorical variables, and finally scale features if needed.","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Handle Missing Values\nFirst, let's check again for missing values and decide how to handle them. We'll fill or drop missing values as appropriate.","metadata":{}},{"cell_type":"code","source":"# Handle missing values (example: fill numeric with median, categorical with mode)\nfor col in train.columns:\n    if train[col].isnull().sum() > 0:\n        if train[col].dtype == 'object':\n            mode = train[col].mode()[0]\n            train[col].fillna(mode, inplace=True)\n            test[col].fillna(mode, inplace=True)\n        else:\n            median = train[col].median()\n            train[col].fillna(median, inplace=True)\n            test[col].fillna(median, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2. Encode Categorical Variables\nNext, we'll convert any categorical features into numeric format using one-hot encoding.","metadata":{}},{"cell_type":"code","source":"# One-hot encode categorical variables\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\ntrain_encoded = pd.get_dummies(train, columns=categorical_cols)\ntest_encoded = pd.get_dummies(test, columns=categorical_cols)\n\n# Align train and test dataframes to have the same columns\ntrain_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3. Feature Scaling (Optional)\nFor some models, scaling features can improve performance. We'll use StandardScaler as an example.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Identify feature columns (exclude target and any ID columns)\ntarget_col = 'BeatsPerMinute'  # Update if your target column is named differently\nfeature_cols = [col for col in train_encoded.columns if col != target_col]\n\nscaler = StandardScaler()\ntrain_encoded[feature_cols] = scaler.fit_transform(train_encoded[feature_cols])\ntest_encoded[feature_cols] = scaler.transform(test_encoded[feature_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Build a Baseline Model\nLet's train a simple regression model as a baseline. We'll use a Random Forest Regressor and evaluate its performance using cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T04:31:53.391754Z","iopub.execute_input":"2025-09-09T04:31:53.392124Z","iopub.status.idle":"2025-09-09T04:31:53.396344Z","shell.execute_reply.started":"2025-09-09T04:31:53.3921Z","shell.execute_reply":"2025-09-09T04:31:53.395394Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Split features and target\nX = train_encoded[feature_cols]\ny = train_encoded[target_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T04:35:02.171303Z","iopub.execute_input":"2025-09-09T04:35:02.171662Z","iopub.status.idle":"2025-09-09T04:35:02.19507Z","shell.execute_reply.started":"2025-09-09T04:35:02.171639Z","shell.execute_reply":"2025-09-09T04:35:02.194259Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\n\n# # Faster Random Forest for testing\n# rf = RandomForestRegressor(n_estimators=10, random_state=42)\n\n# # Faster cross-validation\n# scores = cross_val_score(rf, X, y, cv=3, scoring='neg_root_mean_squared_error')\n# print('Cross-validated RMSE (fast version):', -scores.mean())\n\n# # Fit the model on the full training data\n# rf.fit(X, y)\n\n# # Predict on the test set\n# test_preds = rf.predict(test_encoded[feature_cols])\n\n# # Prepare the submission DataFrame (make sure the column name matches sample_submission)\n# submission = sample_submission.copy()\n# submission['BeatsPerMinute'] = test_preds  # Update column name if needed\n\n# # Save to CSV for Kaggle submission\n# submission.to_csv('submission.csv', index=False)\n# print(\"Submission file 'submission.csv' created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T04:32:00.452293Z","iopub.execute_input":"2025-09-09T04:32:00.452968Z","iopub.status.idle":"2025-09-09T04:32:00.457135Z","shell.execute_reply.started":"2025-09-09T04:32:00.452942Z","shell.execute_reply":"2025-09-09T04:32:00.455992Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## 5. Build a LightGBM Model\nLet's train a LightGBM model. To see its improvement over Random Forest Regressor.","metadata":{"execution":{"iopub.status.busy":"2025-09-09T04:32:07.933576Z","iopub.execute_input":"2025-09-09T04:32:07.933959Z","iopub.status.idle":"2025-09-09T04:32:07.939974Z","shell.execute_reply.started":"2025-09-09T04:32:07.933925Z","shell.execute_reply":"2025-09-09T04:32:07.938762Z"}}},{"cell_type":"code","source":"# !pip install lightgbm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T04:32:22.179704Z","iopub.execute_input":"2025-09-09T04:32:22.180083Z","iopub.status.idle":"2025-09-09T04:32:22.184452Z","shell.execute_reply.started":"2025-09-09T04:32:22.180055Z","shell.execute_reply":"2025-09-09T04:32:22.183446Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import lightgbm as lgb\n\n# Create the LightGBM regressor\nlgbm = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n\n# Cross-validation (same as before, 3 folds, negative RMSE)\nlgbm_scores = cross_val_score(lgbm, X, y, cv=3, scoring='neg_root_mean_squared_error')\nprint('LightGBM Cross-validated RMSE:', -lgbm_scores.mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T04:35:06.400516Z","iopub.execute_input":"2025-09-09T04:35:06.400855Z","iopub.status.idle":"2025-09-09T04:35:11.216313Z","shell.execute_reply.started":"2025-09-09T04:35:06.400829Z","shell.execute_reply":"2025-09-09T04:35:11.21535Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 349442, number of used features: 10\n[LightGBM] [Info] Start training from score 119.053707\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005590 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 349443, number of used features: 10\n[LightGBM] [Info] Start training from score 118.999012\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005750 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 349443, number of used features: 10\n[LightGBM] [Info] Start training from score 119.051980\nLightGBM Cross-validated RMSE: 26.484066344666058\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Fit LightGBM on the full training data\nlgbm.fit(X, y)\n\n# Predict on the test set\nlgbm_test_preds = lgbm.predict(test_encoded[feature_cols])\n\n# Prepare the submission DataFrame (make sure the column name matches sample_submission)\nlgbm_submission = sample_submission.copy()\nlgbm_submission['BeatsPerMinute'] = lgbm_test_preds  # Update column name if needed\n\n# Save to CSV for Kaggle submission\nlgbm_submission.to_csv('submission.csv', index=False)\nprint(\"Submission file from lgbm model 'submission.csv' created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T04:35:11.217422Z","iopub.execute_input":"2025-09-09T04:35:11.21766Z","iopub.status.idle":"2025-09-09T04:35:13.805067Z","shell.execute_reply.started":"2025-09-09T04:35:11.217642Z","shell.execute_reply":"2025-09-09T04:35:13.803955Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030257 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 524164, number of used features: 10\n[LightGBM] [Info] Start training from score 119.034899\nSubmission file from lgbm model 'submission.csv' created!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}